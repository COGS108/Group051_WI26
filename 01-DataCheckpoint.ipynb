{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "TEAM LIST AND CREDITS\n",
    "\n",
    "Eljin Vedar\n",
    "Writing – original draft, Writing –  review & editing \n",
    "\n",
    "Isaac Chen\n",
    "Writing – original draft, Writing –  review & editing\n",
    "\n",
    "Jiafan Du\n",
    "Writing – original draft, Writing –  review & editing\n",
    "\n",
    "Robin Villareal\n",
    "Writing – original draft, Writing –  review & editing \n",
    "\n",
    "Barry Tunstall\n",
    "Writing – original draft, Writing –  review & editing \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“How well does performance on a Dance Dance Revolution chart with difficulty X, defined by the game’s numeric rating, predict a player’s overall skill level across songs of varying difficulty?”\n",
    "\n",
    "\n",
    "For this idea, we aim to address the new player approach to finding a difficulty range. For our rhythm game of interest, we will choose Dance Dance Revolution (DDR), a classic rhythm game. Starting out, it may feel like the game is too easy or too hard. Given pre-existing game stats, we will analyze the data given by a player to make a prediction of their current skill range, or what difficulty X number a player can attempt on average. Furthermore, with this prediction, we can go further into recommending new songs for a player to try out relative to their corresponding difficulty X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As members of the Dance and Rhythm Game Club, we stand to address socioeconomic factors by making arcade rhythm games accessible. We understand the constraints of accessibility to an arcade, time, and disposable income play in being able to experience arcade rhythm games like Dance Dance Revolution (DDR) . It is one of our motivations to implore the general public to try DDR as we bring the arcade experience to campus. As we seek to expand our community, one of the problems we face when reaching out to new players is the willingness to try the game i.e not knowing where to begin. We run into phrases such as “I’m not good”, “I’ve never played before”, and even “I don’t even know where to start!” constantly. To that, we want to devise a method that involves a beginner friendly approach; easing players into DDR assuming they have no experience but an eagerness to learn. For the project, we used three sources based on their relevance to predictive analysis to form the foundation of our research.\n",
    "\n",
    " The paper, “Dance Dance Convolution,” [3](#ref3) is a study that attempts to generate DDR step charts based on machine learning, with two inputs - an audio file, and a desired difficulty. The model would then generate a fully playable chart by decomposing the task into two sequential prediction problems: first determining when steps should occur, and then determining which arrows should be placed at each predicted timing. The results were analyzed based on how similar they were to the original chart - things like average hold note length, arrow selection, and beat placement. The important conclusions that come from the study was the capacity to assign chart difficulty ratings for procedurally generated charts based on note densities, although limited to a max bpm of 120; expanded on in a revisit in 2025 [1](#ref1) which includes a refined version exceeding the 120 bpm threshold. Another useful prediction scheme was being able to predict the chart difficulty [2](#ref2) based on attributes we have assigned as step data and technique data. \n",
    "The main takeaway from the results of the paper, for our uses, is the conclusion that DDR charting aligns closely with underlying musical structure, and that difficulty can be linearly correlated with step frequency, rhythmic density, and other computationally extractable metrics. This is useful for our predictive modeling because it suggests that the player performance data we gather, such as timing accuracy, hit‑window distribution, and error frequency can also be correlated with chart difficulty in a uniform way. Thus, the same mathematical relationships that allow systems like DDC to infer difficulty from audio and chart context can be inverted in our analysis - we can infer a player's effective skill level by measuring how their accuracy degrades or holds steady across segments of varying structural difficulty.\n",
    "\n",
    "The “Dance Dance ConvLSTM” [1](#ref1) paper is an extension of this paper, diving deeper into subsections of charting in order to further quantify patterns that arise in step charting - correlated to chart triggers such as music speedups, slowdowns, or certain step patterns, such as repeated notes on the same arrow - these are usually considered to be more difficult to execute than standard patterning. [2](#ref2) This provides a further foundation for our analysis: if machine‑learning models can reliably infer difficulty from musical and structural cues, then player‑performance data should likewise reflect predictable responses to those same cues, enabling skill prediction through our structural‑difficulty analysis.\n",
    "\n",
    "The third paper, “Predicting Chart Difficulty in Rhythm Games Through Classification Using Chart Pattern Derived Attributes,” also deals with the subject of correlating the in-game difficulty numbers with actual difficulty. Their analysis deals with difficulty and correlating it with chart structure - The main conclusions from the paper, useful for our purposes, is that objective structural difficulty can be quantified and measured, and that the charting in the game’s maps is correlated with this difficulty. [3](#ref3) This is useful for us because it means that we can apply a similar analysis to player skill, since performance data (such as timing accuracy, error rate, and stamina degradation, etc.) should correlate with these same difficulty signifiers in the charts. Essentially, If difficulty can be reliably inferred from chart patterns, then player capability can be inferred from how well a player handles those patterns.\n",
    "\n",
    "From the articles and our own playing experience, we found 3 distinct data categories relevant to our project:\n",
    "1. Song Data (exact song) - data showing the title, artist, and folder the song is located in. This is important because there are versions of DDR that run on a different difficulty scale; making sure the songs user play are in the arcade version of the game.\n",
    "\n",
    "2. Chart Data (Difficulty X) - The degree of gameplay a player can encounter during a song. Individual songs in DDR can be played on varying difficulty levels. Each difficulty iteration is commonly referred to as a chart. Charts are given numerical values assigned by DDR from 1-20. As the number increases, players can expect an increase of notes on the screen (note density) and rhythmic patterns. Charts of equal value have the tendency to feature similar complexity. \n",
    "\n",
    "3. Performance data (how a player does) - the overall score an individual obtains along with a timeline of where the player missed notes. This works in tandem with step data and technique data to evaluate what core fundamentals the person has/needs improvement on. In our project, we will use some of DDR's iconic scoring systems to assess new players (more info in HYPOTHESIS)\n",
    "\n",
    "\n",
    "\n",
    "Although these articles make great strides in the development of generated difficulty, we find the limitation of these contexts to engage with the new player community and the base game of DDR. Having the opportunity to expand on these works, we want to take chart predictions and the data acquired to contextualize them for the use of a new player; giving them a direction as they start their rhythm game journey.\n",
    "\n",
    "1. <a name=\"ref1\"> </a> O’Malley, Miguel. Dance Dance ConvLSTM. 2025, arXiv:2507.01644. https://arxiv.org/pdf/2507.01644\n",
    "2. <a name=\"ref2\"> </a>Caronongan, Arturo P., III, and Nelson A. Marcos. “Predicting Chart Difficulty in Rhythm Games Through Classification Using Chart Pattern Derived Attributes.” Computational Science and Technology, 2021, pp. 193–205. Springer Nature. https://link.springer.com/chapter/10.1007/978-981-33-4069-5_17\n",
    "3. <a name=\"ref3\"> </a>Donahue, Chris, Zachary C. Lipton, and Julian McAuley. “Dance Dance Convolution.” Proceedings of the 34th International Conference on Machine Learning, PMLR 70, 2017. https://proceedings.mlr.press/v70/donahue17a/donahue17a.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As players take on higher difficulty X ratings, DDR charts demand greater stamina, precision, and pattern recognition due to chart complexity increasing (i.e total stepcount, rhythms implemented, how many notes appear at once). Therefore, players who achieve EX% around the community avg EX% reflect the capacity to comprehend and execute these factors. We come to this conclusion since chart complexity can be generalized across songs of equal difficulty X rating; reflecting their underlying skill. \n",
    "\n",
    "\n",
    "We predict a strong correlation between a player's EX% reflecting their overall skill level when compared to the avg EX%.\n",
    "\n",
    "\n",
    "To analyze the skill of a player, we have used/adjusted preexisting variables DDR currently implements to assess players:\n",
    "1. Difficulty X - As stated previously, DDR difficulty has a rating numbered from easiest to hardest: 1-20. DDR includes 4 standard labels: BEGINNER 1-4 , BASIC 3-7, DIFFICULT 5-9, EXPERT 10-13+. One issue that has arisen was the overlapping ranges in the standard labels. As an example, one song can have a BASIC 7, while another song can have a DIFFICULT 7 even though there is no major difficulty spike between the two. For a new player, seeing DIFFICULT 7 instead of BASIC 7 may afflict a bias when choosing the difficulty. To avoid ambiguity, we want to simplify the system purely on the numeric RATING; assigning it to our variable, X (X can range from 1-20). When we refer to difficulty X now, it provides a clearer representation of difficulty while still utilizing the pre-existing DDR system. (a 7 is a 7!).\n",
    "\n",
    "\n",
    "2. EX - EX is a scoring system originating from DDR EXTREME; independent from the in game combo and life bar players use. EX was designed to be a timing-based accuracy score, given by the summation of the 2 best timing judgements - Marvelous and Perfect. To calculate EX, the formula is given below:\n",
    "EX = ∑ 2(Marvelous) + 1(Perfect)\n",
    "\n",
    "\n",
    "3. EX% - EX% is EX normalized. Because charts have varying total notecounts EX alone is\n",
    "Not enough to determine overall skill level. By normalizing EX, we obtain a metric that allows us to compare charts of different total notecounts. To calculate EX% we use the formula:\n",
    "\tEX% = \tEX/(3 x Total Nolecount)\n",
    "\n",
    "\n",
    "4. Average EX% (avg EX%)  - The average EX% on a given song that will give us the baseline for a player's performance in relation to players who attempted the same song and chart difficulty X. Using community-driven data, our team has acquired the avg EX% we will use to compare a subset of DDR players to. We understand that one of the constraints of community driven data is the possibility of inflated averages due to the data coming from registered users. These registered users may offer a higher level of time commitment than unregistered users (our subject of interests). For our project avg EX% is used to operationalize performance when we assign a player skill level via difficulty X. avg EX% is calculated through the following formula:\n",
    "\tavg EX% = ∑EX% across all players/number of players\n",
    "\n",
    "\n",
    "5. Overall skill level - A player's overall skill level is operationalized as their individual EX% on a difficulty X chart in relation to the given difficulty X’s avg EX%.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Week|Main Task|Due Date| \n",
    "|:---:|---|:---:|\n",
    "|5 (2/4)| Edit, finalize, and submit proposal<br>Search for datasets|Proposal Due (2/4)| \n",
    "|6 (2/11)| Data Checkpoint:<br>• Animefest this weekend, acquire sample<br>• Wrangle data of references<br>• Logistics as specified in proposal<br>• Make sure we have a way of obtaining necessary variables<br>| N/A|\n",
    "|7 (2/18)|Edit, finalize, and submit Data checkpoint|Data Checkpoint Revisions (2/18)|\n",
    "|8 (2/25)|EDA checkpoint:<br>• Substantive analysis<br>• Multiple plots<br>• Summary statistics<br>• Early insights<br>• Potential issues (outliers, skew, missingness) |N/A|\n",
    "|9 (3/4)|Edit, finalize, and submit EDA checkpoint|EDA checkpoint Revisions (3/4)|\n",
    "|10 (3/11)|Video|N/A|\n",
    "|11 (3/18)|Edit, finalize, and submit Project|Whole Project + Video|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
